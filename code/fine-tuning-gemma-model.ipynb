{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Install the required libraries**","metadata":{}},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes==0.42.0\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-09T15:08:51.734060Z","iopub.execute_input":"2024-07-09T15:08:51.734399Z","iopub.status.idle":"2024-07-09T15:10:28.638503Z","shell.execute_reply.started":"2024-07-09T15:08:51.734369Z","shell.execute_reply":"2024-07-09T15:10:28.637244Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**Import the required libraries**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import LoraConfig\nimport os\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import notebook_login\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:28:20.271650Z","iopub.execute_input":"2024-07-09T15:28:20.272348Z","iopub.status.idle":"2024-07-09T15:28:21.625010Z","shell.execute_reply.started":"2024-07-09T15:28:20.272319Z","shell.execute_reply":"2024-07-09T15:28:21.624367Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#lora config\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:16:14.908219Z","iopub.execute_input":"2024-07-09T15:16:14.909026Z","iopub.status.idle":"2024-07-09T15:16:14.913367Z","shell.execute_reply.started":"2024-07-09T15:16:14.908996Z","shell.execute_reply":"2024-07-09T15:16:14.912452Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#bits and bytes config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:15:52.472643Z","iopub.execute_input":"2024-07-09T15:15:52.473452Z","iopub.status.idle":"2024-07-09T15:15:52.479048Z","shell.execute_reply.started":"2024-07-09T15:15:52.473421Z","shell.execute_reply":"2024-07-09T15:15:52.478243Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#gemma 2b\nmodel_id = \"google/gemma-2b-it\"","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:16:19.466896Z","iopub.execute_input":"2024-07-09T15:16:19.467577Z","iopub.status.idle":"2024-07-09T15:16:19.471732Z","shell.execute_reply.started":"2024-07-09T15:16:19.467546Z","shell.execute_reply":"2024-07-09T15:16:19.470794Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"token\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:23:27.684936Z","iopub.execute_input":"2024-07-09T15:23:27.685576Z","iopub.status.idle":"2024-07-09T15:23:27.948100Z","shell.execute_reply.started":"2024-07-09T15:23:27.685544Z","shell.execute_reply":"2024-07-09T15:23:27.947531Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"notebook_login(token)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:23:29.327580Z","iopub.execute_input":"2024-07-09T15:23:29.327940Z","iopub.status.idle":"2024-07-09T15:23:29.347882Z","shell.execute_reply.started":"2024-07-09T15:23:29.327910Z","shell.execute_reply":"2024-07-09T15:23:29.347249Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff6392f152d43de834bf69504ddc801"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n                        model_id, \n                        quantization_config=bnb_config, \n                        device_map={\"\":0}, \n                        token=token)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:23:44.811163Z","iopub.execute_input":"2024-07-09T15:23:44.811545Z","iopub.status.idle":"2024-07-09T15:25:10.535955Z","shell.execute_reply.started":"2024-07-09T15:23:44.811515Z","shell.execute_reply":"2024-07-09T15:25:10.535388Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c09f81772214be38069d5be87231104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe04e3a84b74768817e8a9cb2c0b007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011ac2255821463ba2161296a162c5c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b492c5e2a5a34ea8a1ff5a94585b2c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae20ab1a39247d68321b61fea2a21c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3528bee0bfe471d9932d48e68840594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bea22dbf2f04dec94fd38569d7e0bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b913cd054f48d8a56c7fc05b596b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3d7ab498d84f5b8eba334dcea74116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2441afd16a48f2b832eb71bb1e7d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6ee1cae46b4e399364f47764aea4d3"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"<start_of_turn>user\nExplain 'AMSI init bypass' and its purpose.<end_of_turn>\n<start_of_turn>model\"\"\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=50)\nout = tokenizer.decode(outputs[0], skip_special_tokens=True)\nout","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:12.676008Z","iopub.execute_input":"2024-07-09T15:27:12.676374Z","iopub.status.idle":"2024-07-09T15:27:12.681617Z","shell.execute_reply.started":"2024-07-09T15:27:12.676345Z","shell.execute_reply":"2024-07-09T15:27:12.680888Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"user\\nExplain 'AMSI init bypass' and its purpose.\\nmodelSure, here's a detailed explanation of the AMSI init bypass feature and its purpose:\\n\\n**AMSI init bypass:**\\n\\nAMSI (Advanced Microcontroller Startup Interface) init bypass is a technique used in microcontroller initialization where the initialization process is\""},"metadata":{}}]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:22.806335Z","iopub.execute_input":"2024-07-09T15:27:22.807050Z","iopub.status.idle":"2024-07-09T15:27:22.810763Z","shell.execute_reply.started":"2024-07-09T15:27:22.807018Z","shell.execute_reply":"2024-07-09T15:27:22.810009Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#load the dataset\ndata = load_dataset(\"ahmed000000000/cybersec\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:24.936020Z","iopub.execute_input":"2024-07-09T15:27:24.936678Z","iopub.status.idle":"2024-07-09T15:27:31.387913Z","shell.execute_reply.started":"2024-07-09T15:27:24.936645Z","shell.execute_reply":"2024-07-09T15:27:31.387329Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/9.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7cf2d6c6efa4d42aa58acd6457c7607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f82546de947b4e3a9b9815f38418810e"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:34.534385Z","iopub.execute_input":"2024-07-09T15:27:34.535275Z","iopub.status.idle":"2024-07-09T15:27:34.540400Z","shell.execute_reply.started":"2024-07-09T15:27:34.535220Z","shell.execute_reply":"2024-07-09T15:27:34.539829Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['INSTRUCTION', 'RESPONSE'],\n        num_rows: 12408\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"f\"\"\"input: {data['train']['INSTRUCTION'][2]}\\noutput: {data['train']['RESPONSE'][2]}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:50.997283Z","iopub.execute_input":"2024-07-09T15:27:50.997902Z","iopub.status.idle":"2024-07-09T15:27:51.049359Z","shell.execute_reply.started":"2024-07-09T15:27:50.997868Z","shell.execute_reply":"2024-07-09T15:27:51.048633Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"\"input: Define 'Clickjacking' and describe how it can be prevented.\\noutput: Clickjacking is a type of attack where a malicious website overlays an invisible iframe on top of another website, tricking the user into clicking on a hidden button or link. This can lead to unintended actions, such as sharing sensitive information or making unauthorized purchases. To prevent clickjacking, websites can use the X-Frame-Options header to explicitly tell web browsers whether or not the website should be displayed within an iframe. This header can be set to 'deny' to prevent any framing, 'sameorigin' to allow framing by the same origin, or to a specific URI to allow limited framing by a trusted source.\""},"metadata":{}}]},{"cell_type":"code","source":"def formatting_func(example):\n    text = f\"<start_of_turn>user\\n{example['INSTRUCTION'][0]}<end_of_turn> <start_of_turn>model\\n{example['RESPONSE'][0]}<end_of_turn>\"\n    return [text]","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:27:58.390760Z","iopub.execute_input":"2024-07-09T15:27:58.391124Z","iopub.status.idle":"2024-07-09T15:27:58.395171Z","shell.execute_reply.started":"2024-07-09T15:27:58.391091Z","shell.execute_reply":"2024-07-09T15:27:58.394415Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=150,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_func,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:28:28.657169Z","iopub.execute_input":"2024-07-09T15:28:28.658198Z","iopub.status.idle":"2024-07-09T15:28:30.595991Z","shell.execute_reply.started":"2024-07-09T15:28:28.658149Z","shell.execute_reply":"2024-07-09T15:28:30.595242Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f5b404bac44dac953609b299188b8a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:28:59.105579Z","iopub.execute_input":"2024-07-09T15:28:59.105955Z","iopub.status.idle":"2024-07-09T15:34:34.581979Z","shell.execute_reply.started":"2024-07-09T15:28:59.105925Z","shell.execute_reply":"2024-07-09T15:34:34.581342Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 05:32, Epoch 46/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.563000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>4.065400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4.021600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.608900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.313200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.612800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.727500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.234400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.592700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.327100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.367000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.353200</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.297900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.272300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.915900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.782400</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.552600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.503700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.847700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.546900</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.405700</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.132300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.108800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.928800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.976700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.080100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.729700</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.869500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.581400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.685000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.607100</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.428900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.402500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.410400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.378700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.310000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.313700</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.268200</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.291500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.269200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.217300</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.238700</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.207100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.184700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.193400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.189000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.188100</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.161800</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.184600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.188600</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.142900</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.157600</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.134700</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.130000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.155700</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.160500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.132400</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.132800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.130100</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.138300</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.133000</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.134600</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.133000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.130000</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.139200</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.104800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.126800</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.127400</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.114400</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.120300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.101900</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.118100</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.109100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.108200</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.097900</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.110700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.113500</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.091100</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.110000</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.110700</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.099400</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.101000</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.106100</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.102300</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.106500</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.081800</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.109800</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.090100</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.097000</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.075900</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.097800</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.103400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.082700</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.087500</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.081900</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.093100</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.090500</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.087300</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.076300</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.078600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.078700</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.088900</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.068800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.072800</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.087700</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.067800</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.076800</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.081100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.080400</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.072600</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.073100</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.071100</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.079700</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.062900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.082500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=150, training_loss=0.5246245095630487, metrics={'train_runtime': 335.0063, 'train_samples_per_second': 1.791, 'train_steps_per_second': 0.448, 'total_flos': 1065426810224640.0, 'train_loss': 0.5246245095630487, 'epoch': 46.15})"},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"<start_of_turn>user\nExplain 'AMSI init bypass' and its purpose.<end_of_turn>\n<start_of_turn>model\"\"\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=50)\ntokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:35:34.727077Z","iopub.execute_input":"2024-07-09T15:35:34.727935Z","iopub.status.idle":"2024-07-09T15:35:38.630257Z","shell.execute_reply.started":"2024-07-09T15:35:34.727904Z","shell.execute_reply":"2024-07-09T15:35:38.629499Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"\"user\\nExplain 'AMSI init bypass' and its purpose.\\nmodel\\nAMSI init bypass is a security feature in Windows that allows the System Management Interface (AMSI) to be initialized without being properly protected. This feature is designed to provide some protection against certain types of attacks, by preventing unauthorized users from tampering with\""},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"<start_of_turn>user\nExplain 'APT groups and operations' and its purpose.<end_of_turn>\n<start_of_turn>model\"\"\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=50)\ntokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:35:42.709655Z","iopub.execute_input":"2024-07-09T15:35:42.710317Z","iopub.status.idle":"2024-07-09T15:35:46.483913Z","shell.execute_reply.started":"2024-07-09T15:35:42.710285Z","shell.execute_reply":"2024-07-09T15:35:46.483240Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"\"user\\nExplain 'APT groups and operations' and its purpose.\\nmodel\\nAPT groups and operations refer to the organization and grouping of security updates and countermeasures in an Automated Patch Tool (APT) system. By grouping similar items together, it is possible to define rules and restrictions that apply to entire groups of systems or devices\""},"metadata":{}}]},{"cell_type":"markdown","source":"**In the above cells the model generated some text related to the query asked.**","metadata":{}}]}